{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from det3d.models.centerpoint import CenterPoint\n",
    "from det3d.types.pointcloud import PointCloud\n",
    "from det3d.utils import move_to_gpu, move_to_cpu, print_dict_tensors_size\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import glob\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 定义 Dataset（与模型输出保持一致）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(batch_data):\n",
    "    \"\"\"\n",
    "    batch_data: list of samples, 每个样本格式为 (pc, label)\n",
    "        其中 pc 为一个对象，其属性 points 为 numpy array (N_i, feature_dim)\n",
    "        label 为该帧对应的标签数据（例如 gt dict 等）。\n",
    "    \n",
    "    返回：\n",
    "        padded_points: shape [batch_size, max_num_points, feature_dim+1]\n",
    "           —— 第一列为 batch_index, 后面的列为原始特征\n",
    "        labels: 列表形式保存每个样本的 label\n",
    "        lengths: 每帧原始点数，方便后续做 mask\n",
    "    \"\"\"\n",
    "    point_clouds = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    # 遍历每个样本，同时记录下样本在 batch 中的 index\n",
    "    for batch_idx, sample in enumerate(batch_data):\n",
    "        # sample[0].points 是一个 numpy array，形状为 (N_i, feature_dim)\n",
    "        points_tensor = torch.tensor(sample[0].points, dtype=torch.float)\n",
    "        lengths.append(points_tensor.shape[0])\n",
    "        labels.append(sample[1])\n",
    "        # 为当前样本所有的点创建 batch index 列\n",
    "        batch_idx_tensor = torch.full((points_tensor.size(0), 1), batch_idx, dtype=torch.float)\n",
    "        # 拼接，得到新的 tensor，形状为 (N_i, feature_dim + 1)\n",
    "        points_tensor = torch.cat([batch_idx_tensor, points_tensor], dim=1)\n",
    "        point_clouds.append(points_tensor)\n",
    "    \n",
    "    # 使用 pad_sequence 对齐所有点云，padding_value=0\n",
    "    padded_points = pad_sequence(point_clouds, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_points, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        voxel_size: list,          # [voxel_x, voxel_y, voxel_z] (这里只使用 x, y)\n",
    "        feature_map_stride: int,\n",
    "        num_classes: int,\n",
    "        pc_range: list,\n",
    "        debug: bool = False        # 调试模式，默认 False\n",
    "    ):\n",
    "        self.num_samples = 100\n",
    "        self.num_points = 1000\n",
    "        self.voxel_size = voxel_size\n",
    "        self.feature_map_stride = feature_map_stride\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = debug\n",
    "\n",
    "        self.label_mapping = {\n",
    "            \"Car\": 0,\n",
    "            \"Cyclist\": 1,\n",
    "            \"Pedestrian\": 2,\n",
    "            # \"Hero\": 3\n",
    "        }\n",
    "\n",
    "        # ------------------------------\n",
    "        # 读取本地点云数据\n",
    "        # ------------------------------\n",
    "        pc_path = '/workspace/rosbags/archive/data_city/data_city/lidar_livox'\n",
    "        npy_files = glob.glob(f\"{pc_path}/*.npy\")\n",
    "        self.pc_list = [np.load(file) for file in npy_files]\n",
    "\n",
    "        # 使用预设的 pc_range\n",
    "        self.point_cloud_range = pc_range\n",
    "        if self.debug:\n",
    "            print(f'[DEBUG] Computed point_cloud_range: {self.point_cloud_range}')\n",
    "\n",
    "        # ------------------------------\n",
    "        # 读取本地标签数据\n",
    "        # ------------------------------\n",
    "        label_path = '/workspace/rosbags/archive/data_city/data_city/label3'\n",
    "        labels = glob.glob(f\"{label_path}/*.txt\")\n",
    "        label_list = []\n",
    "        for label_file in labels:\n",
    "            gt_dict = dict(\n",
    "                gt_boxes = [],\n",
    "                gt_labels = [],\n",
    "            )\n",
    "            with open(label_file, \"r\") as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    raw_line_list = line.split(\" \")\n",
    "                    # 前7个数字对应 [x, y, z, l, w, h, rot]\n",
    "                    xyzlwhr = torch.Tensor([float(x) for x in raw_line_list[:-1]])\n",
    "                    cur_label = raw_line_list[-1].replace('\\n','').strip()\n",
    "                    if cur_label == \"Hero\":\n",
    "                        continue\n",
    "                    gt_dict[\"gt_boxes\"].append(xyzlwhr)\n",
    "                    gt_dict[\"gt_labels\"].append(self.label_mapping[cur_label])\n",
    "            if len(gt_dict[\"gt_boxes\"]) > 0:\n",
    "                gt_dict[\"gt_boxes\"] = torch.stack(gt_dict[\"gt_boxes\"])\n",
    "                gt_dict[\"gt_labels\"] = torch.tensor(gt_dict[\"gt_labels\"], dtype=torch.long)\n",
    "            else:\n",
    "                gt_dict[\"gt_boxes\"] = torch.empty((0, 7))\n",
    "                gt_dict[\"gt_labels\"] = torch.empty((0,), dtype=torch.long)\n",
    "            label_list.append(gt_dict)\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    # ---------- 辅助函数 ----------\n",
    "\n",
    "    def get_bev_size(self):\n",
    "        \"\"\"\n",
    "        根据 point_cloud_range 和 voxel_size 计算 BEV 特征图尺寸 (H, W)。\n",
    "        \"\"\"\n",
    "        x_min, y_min, z_min, x_max, y_max, z_max = self.point_cloud_range\n",
    "        vx, vy = self.voxel_size[0], self.voxel_size[1]\n",
    "        stride = self.feature_map_stride\n",
    "        W = round((x_max - x_min) / (vx * stride))\n",
    "        H = round((y_max - y_min) / (vy * stride))\n",
    "        return H, W\n",
    "\n",
    "    def map_to_bev(self, x, y):\n",
    "        \"\"\"\n",
    "        将 (x, y) 坐标映射到 BEV 特征图中，返回：\n",
    "         - bev_x, bev_y: 浮点数坐标（未取整）\n",
    "         - x_int, y_int: 转换为整数索引（grid cell 坐标）\n",
    "        \"\"\"\n",
    "        x_min, y_min, _, _, _, _ = self.point_cloud_range\n",
    "        vx, vy = self.voxel_size[0], self.voxel_size[1]\n",
    "        stride = self.feature_map_stride\n",
    "        bev_x = (x - x_min) / (vx * stride)\n",
    "        bev_y = (y - y_min) / (vy * stride)\n",
    "        x_int, y_int = int(bev_x), int(bev_y)\n",
    "        return bev_x, bev_y, x_int, y_int\n",
    "\n",
    "    def get_gaussian_patch_indices(self, center_idx, radius, H, W):\n",
    "        \"\"\"\n",
    "        根据中心索引 center_idx = (x_int, y_int) 和高斯核半径 radius，\n",
    "        计算 BEV 区域及高斯核 patch 的索引范围：\n",
    "            h_x_min, h_x_max, h_y_min, h_y_max,\n",
    "            g_x_min, g_x_max, g_y_min, g_y_max\n",
    "        \"\"\"\n",
    "        x_int, y_int = center_idx\n",
    "        left = x_int - radius\n",
    "        right = x_int + radius + 1\n",
    "        top = y_int - radius\n",
    "        bottom = y_int + radius + 1\n",
    "\n",
    "        g_x_min = max(0, -left)\n",
    "        g_x_max = (2 * radius + 1) - max(0, right - W)\n",
    "        g_y_min = max(0, -top)\n",
    "        g_y_max = (2 * radius + 1) - max(0, bottom - H)\n",
    "\n",
    "        h_x_min = max(0, left)\n",
    "        h_x_max = min(W, right)\n",
    "        h_y_min = max(0, top)\n",
    "        h_y_max = min(H, bottom)\n",
    "\n",
    "        return h_x_min, h_x_max, h_y_min, h_y_max, g_x_min, g_x_max, g_y_min, g_y_max\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 加载点云和对应 GT 标签字典\n",
    "        pc = PointCloud(self.pc_list[idx])\n",
    "        gt_dict = self.label_list[idx]\n",
    "\n",
    "        # 对 GT 进行过滤：只保留在 BEV 内的 box\n",
    "        original_boxes = gt_dict['gt_boxes']\n",
    "        original_labels = gt_dict['gt_labels']\n",
    "        H, W = self.get_bev_size()\n",
    "\n",
    "        valid_reg_list = []\n",
    "        valid_ind_list = []\n",
    "        valid_size_list = []\n",
    "        valid_boxes_list = []\n",
    "        valid_labels_list = []\n",
    "        valid_height_list = []\n",
    "        valid_rot_list = []\n",
    "\n",
    "        for i, box in enumerate(original_boxes):\n",
    "            x, y, z_val, l_box, w_box, h_box, rot_val = box.tolist()\n",
    "            bev_x, bev_y, x_int, y_int = self.map_to_bev(x, y)\n",
    "            # 只保留投影落在 [0,W) 和 [0,H) 内的 box\n",
    "            if 0 <= x_int < W and 0 <= y_int < H:\n",
    "                valid_boxes_list.append(box)\n",
    "                valid_labels_list.append(original_labels[i])\n",
    "                offset = [bev_x - x_int, bev_y - y_int]\n",
    "                valid_reg_list.append(offset)\n",
    "                valid_ind_list.append(y_int * W + x_int)\n",
    "                size_w = l_box / (self.voxel_size[0] * self.feature_map_stride)\n",
    "                size_h = w_box / (self.voxel_size[1] * self.feature_map_stride)\n",
    "                size_z = h_box / (self.voxel_size[2] * self.feature_map_stride)\n",
    "                valid_size_list.append([size_w, size_h,size_z])\n",
    "                valid_height_list.append(z_val)\n",
    "                valid_rot_list.append(rot_val)\n",
    "            else:\n",
    "                if self.debug:\n",
    "                    print(f'[DEBUG] Box {i} with center ({x:.2f},{y:.2f}) mapped to ({x_int},{y_int}) is out of BEV range.')\n",
    "\n",
    "        if len(valid_boxes_list) > 0:\n",
    "            filtered_gt_boxes = torch.stack(valid_boxes_list)\n",
    "            filtered_gt_labels = torch.tensor(valid_labels_list, dtype=torch.long)\n",
    "            reg = torch.tensor(valid_reg_list, dtype=torch.float32)      # [valid_num, 2]\n",
    "            ind = torch.tensor(valid_ind_list, dtype=torch.long)           # [valid_num]\n",
    "            size = torch.tensor(valid_size_list, dtype=torch.float32)      # [valid_num, 2]\n",
    "            reg_mask = torch.ones(len(valid_ind_list), dtype=torch.uint8)  # 有效标记\n",
    "            height = torch.tensor(valid_height_list, dtype=torch.float32)  # [valid_num]\n",
    "            rot = torch.tensor(valid_rot_list, dtype=torch.float32)        # [valid_num]\n",
    "        else:\n",
    "            filtered_gt_boxes = torch.empty((0, 7))\n",
    "            filtered_gt_labels = torch.empty((0,), dtype=torch.long)\n",
    "            reg = torch.empty((0, 2), dtype=torch.float32)\n",
    "            ind = torch.empty((0,), dtype=torch.long)\n",
    "            size = torch.empty((0, 3), dtype=torch.float32)\n",
    "            reg_mask = torch.empty((0,), dtype=torch.uint8)\n",
    "            height = torch.empty((0,), dtype=torch.float32)\n",
    "            rot = torch.empty((0,), dtype=torch.float32)\n",
    "\n",
    "        # 生成 GT 热力图（仅用有效的 box）\n",
    "        filtered_heatmap = self.generate_heatmap(\n",
    "            filtered_gt_boxes, filtered_gt_labels,\n",
    "            point_cloud_range=self.point_cloud_range,\n",
    "            voxel_size=self.voxel_size,\n",
    "            feature_map_stride=self.feature_map_stride,\n",
    "            num_classes=self.num_classes\n",
    "        )\n",
    "\n",
    "        # 构造返回的 GT 字典\n",
    "        final_gt = {\n",
    "            'gt_boxes': filtered_gt_boxes,   # 只包含有效 box\n",
    "            'gt_labels': filtered_gt_labels,\n",
    "            'heatmap': filtered_heatmap,\n",
    "            'ind': ind,\n",
    "            'reg': reg,\n",
    "            'reg_mask': reg_mask,\n",
    "            'size': size,\n",
    "            'height': height,\n",
    "            'rot': rot\n",
    "        }\n",
    "        \n",
    "        return pc, final_gt\n",
    "\n",
    "    def gaussian2D(self, shape, sigma=1):\n",
    "        \"\"\"生成 2D 高斯核矩阵\"\"\"\n",
    "        m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "        y, x = np.ogrid[-m:m+1, -n:n+1]\n",
    "        h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
    "        h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "        return h\n",
    "\n",
    "    def gaussian_radius(self, det_size, min_overlap=0.5):\n",
    "        \"\"\"计算高斯核半径（基于目标尺寸）\"\"\"\n",
    "        height, width = det_size\n",
    "        a1 = 1\n",
    "        b1 = (height + width)\n",
    "        c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
    "        sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
    "        r1 = (b1 - sq1) / (2 * a1)\n",
    "        \n",
    "        a2 = 4\n",
    "        b2 = 2 * (height + width)\n",
    "        c2 = (1 - min_overlap) * width * height\n",
    "        sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n",
    "        r2 = (b2 - sq2) / (2 * a2)\n",
    "        \n",
    "        a3 = 4 * min_overlap\n",
    "        b3 = -2 * min_overlap * (height + width)\n",
    "        c3 = (min_overlap - 1) * width * height\n",
    "        sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n",
    "        r3 = (b3 + sq3) / (2 * a3)\n",
    "        \n",
    "        return min(r1, r2, r3)\n",
    "\n",
    "    def generate_heatmap(self, gt_boxes, gt_labels, point_cloud_range, voxel_size,\n",
    "                         feature_map_stride=4, num_classes=3):\n",
    "        \"\"\"\n",
    "        根据 gt_boxes 和 gt_labels 生成形状为 [num_classes, H, W] 的 heatmap，\n",
    "        H,W 与 get_bev_size() 计算一致。\n",
    "        \"\"\"\n",
    "        dx, dy = voxel_size[0], voxel_size[1]\n",
    "        H, W = self.get_bev_size()\n",
    "        if self.debug:\n",
    "            print(f'[DEBUG] BEV feature map size: H={H}, W={W}')\n",
    "\n",
    "        heatmap = torch.zeros((num_classes, H, W))\n",
    "        for i, (box, label) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "            x, y, _, dx_size, dy_size, _, _ = box.tolist()\n",
    "            bev_x, bev_y, x_int, y_int = self.map_to_bev(x, y)\n",
    "            if self.debug:\n",
    "                print(f'[DEBUG] Box {i}: center=({x:.2f}, {y:.2f}), mapped BEV=({bev_x:.2f}, {bev_y:.2f}) -> (x_int, y_int)=({x_int}, {y_int}), label={label}')\n",
    "            if not (0 <= x_int < W and 0 <= y_int < H):\n",
    "                if self.debug:\n",
    "                    print(f'[DEBUG] Box {i} is out of BEV bounds, skipped.')\n",
    "                continue\n",
    "            # 计算 box 在 BEV 上的尺寸（顺序与 voxel_size 保持一致）\n",
    "            box_hw = (dy_size / dy / feature_map_stride, dx_size / dx / feature_map_stride)\n",
    "            radius = self.gaussian_radius(box_hw)\n",
    "            radius = max(0, int(radius))\n",
    "            diameter = 2 * radius + 1\n",
    "            if self.debug:\n",
    "                print(f'[DEBUG] Box {i}: box_hw={box_hw}, gaussian radius={radius}, diameter={diameter}')\n",
    "            \n",
    "            gaussian = self.gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
    "            gaussian = torch.from_numpy(gaussian).float()\n",
    "            \n",
    "            h_x_min, h_x_max, h_y_min, h_y_max, g_x_min, g_x_max, g_y_min, g_y_max = \\\n",
    "                self.get_gaussian_patch_indices((x_int, y_int), radius, H, W)\n",
    "            \n",
    "            if self.debug:\n",
    "                print(f'[DEBUG] Box {i}: BEV region: x[{h_x_min}:{h_x_max}], y[{h_y_min}:{h_y_max}]')\n",
    "                print(f'[DEBUG] Box {i}: Gaussian patch indices: x[{g_x_min}:{g_x_max}], y[{g_y_min}:{g_y_max}]')\n",
    "            \n",
    "            masked_heatmap = heatmap[label, h_y_min:h_y_max, h_x_min:h_x_max]\n",
    "            masked_gaussian = gaussian[g_y_min:g_y_max, g_x_min:g_x_max]\n",
    "            if self.debug:\n",
    "                print(f'[DEBUG] Box {i}: before update, heatmap sum={masked_heatmap.sum():.4f}, gaussian sum={masked_gaussian.sum():.4f}')\n",
    "            if masked_gaussian.shape != masked_heatmap.shape:\n",
    "                if self.debug:\n",
    "                    print(f'[DEBUG] Box {i}: Shape mismatch: heatmap patch shape {masked_heatmap.shape}, gaussian patch shape {masked_gaussian.shape}. Skipping this box.')\n",
    "                continue\n",
    "            heatmap[label, h_y_min:h_y_max, h_x_min:h_x_max] = torch.maximum(masked_heatmap, masked_gaussian)\n",
    "            if self.debug:\n",
    "                print(f'[DEBUG] Box {i}: after update, new heatmap channel {label} sum={heatmap[label].sum():.4f}')\n",
    "        \n",
    "        if self.debug:\n",
    "            import matplotlib.pyplot as plt\n",
    "            fig, axs = plt.subplots(1, num_classes, figsize=(5 * num_classes, 4))\n",
    "            if num_classes == 1:\n",
    "                axs = [axs]\n",
    "            for i in range(num_classes):\n",
    "                heat = heatmap[i].numpy()\n",
    "                im = axs[i].imshow(heat, cmap='hot', interpolation='nearest')\n",
    "                axs[i].set_title(f\"Heatmap for class {i}\")\n",
    "                fig.colorbar(im, ax=axs[i])\n",
    "            plt.suptitle(\"All Channel Heatmaps\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到dataset中每个data的input为一帧点云 (N,4), gt为boxes,labels,scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 使用提供的CenterPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CenterPoint([0.2,0.2,0.2],[0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "# summary(model,input_size=(2,10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些尺寸实际上正是由各个预测分支的设计决定的。一般来说，CenterPoint 和类似模型会为每个任务分配固定数量的输出通道，然后对整个 BEV 特征图进行卷积预测。具体解释如下：\n",
    "\n",
    "1. **raw_hm (热力图)**  \n",
    "   - 输出尺寸为 `[3, 448, 1120]`，其中 3 表示预测类别数（这里类别数为 3，比如 Vehicle、Pedestrian、Cyclist）。因此，每个类别对应一个通道，最终形成一个形状为 `[num_classes, H, W]` 的热力图。\n",
    "\n",
    "2. **raw_center (中心偏移)**  \n",
    "   - 输出尺寸为 `[2, 448, 1120]`，这里 2 通常表示在 BEV 特征图上，每个 grid cell 内的预测中心坐标（x 和 y 的残差）。即从离散的网格中心到真实中心的偏移。\n",
    "\n",
    "3. **raw_center_z (高度/中心z值)**  \n",
    "   - 输出尺寸为 `[1, 448, 1120]`，只有 1 个通道，用于预测目标在 z 轴方向上的位置（或者说目标的高度信息）。\n",
    "\n",
    "4. **raw_dim (尺寸)**  \n",
    "   - 输出尺寸为 `[3, 448, 1120]`，这里 3 个通道分别对应目标的长、宽、高（l, w, h）的预测值，通常预测的是对数尺度（后续可能会做 exp 处理）或者直接在 BEV 中的尺寸。\n",
    "\n",
    "5. **raw_rot (旋转角度)**  \n",
    "   - 输出尺寸为 `[2, 448, 1120]`，通常用两个通道分别表示旋转角度的 sin 和 cos 值，这样可以避免角度回归带来的周期性问题。\n",
    "\n",
    "**总结：**  \n",
    "- 每个分支的通道数即对应任务的输出数量。  \n",
    "- 448 和 1120 是 BEV 特征图的高度和宽度（由点云范围、voxel 大小和下采样系数决定）。  \n",
    "- 例如，hm 分支输出 3 个通道对应 3 个类别，中心偏移输出 2 个通道（x,y），中心高度（z）输出 1 个通道，尺寸输出 3 个通道，而旋转输出 2 个通道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(pred, gt, alpha=2, beta=4):\n",
    "    \"\"\"\n",
    "    实现一个简单版本的 focal loss，\n",
    "    输入 pred 和 gt 均为 [num_classes, H, W] 的张量\n",
    "    pred 应该是经过 sigmoid 激活后的预测热力图\n",
    "    \"\"\"\n",
    "    assert(isinstance(pred,torch.Tensor))\n",
    "    assert(isinstance(gt, torch.Tensor))\n",
    "    \n",
    "    pos_inds = (gt == 1).float()\n",
    "    neg_inds = (gt < 1).float()\n",
    "\n",
    "    pos_loss = -torch.log(pred + 1e-4) * torch.pow(1 - pred, alpha) * pos_inds\n",
    "    neg_loss = -torch.log(1 - pred + 1e-4) * torch.pow(pred, alpha) * torch.pow(1 - gt, beta) * neg_inds\n",
    "\n",
    "    num_pos = pos_inds.sum()\n",
    "    if num_pos == 0:\n",
    "        loss = neg_loss.sum()\n",
    "    else:\n",
    "        loss = (pos_loss.sum() + neg_loss.sum()) / num_pos\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyz_loss(pred_dicts, gt_dicts):\n",
    "    total_offset_loss = 0.0\n",
    "    total_height_loss = 0.0\n",
    "    batch_count = len(pred_dicts)\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        pred = pred_dicts[i]\n",
    "        gt = gt_dicts[i]\n",
    "        \n",
    "        raw_center = pred[\"raw_center\"]   # shape: [2, H, W]\n",
    "        _, H, W = raw_center.shape\n",
    "        raw_center_flat = raw_center.view(2, -1).transpose(0, 1)  # shape: [H*W, 2]\n",
    "        \n",
    "        raw_center_z = pred[\"raw_center_z\"]  # shape: [1, H, W]\n",
    "        raw_center_z_flat = raw_center_z.view(1, -1).transpose(0, 1)  # shape: [H*W, 1]\n",
    "        \n",
    "        # 确保索引为 long 类型\n",
    "        indices = gt[\"ind\"].to(torch.long)\n",
    "        if indices.nelement() > 0:\n",
    "            pred_offset = raw_center_flat[indices]  # shape: [N, 2]\n",
    "            pred_height = raw_center_z_flat[indices]  # shape: [N, 1]\n",
    "            \n",
    "            gt_offset = gt[\"reg\"]                 # shape: [N, 2]\n",
    "            gt_height = gt[\"height\"].unsqueeze(-1)  # shape: [N, 1]\n",
    "            reg_mask = gt[\"reg_mask\"].float()\n",
    "            num_valid = reg_mask.sum() + 1e-4\n",
    "\n",
    "            offset_loss = F.l1_loss(pred_offset, gt_offset, reduction=\"sum\") / num_valid\n",
    "            height_loss = F.l1_loss(pred_height, gt_height, reduction=\"sum\") / num_valid\n",
    "        else:\n",
    "            # 如果没有有效目标，则构造一个与预测相关联的零 loss 以便 backward 正常\n",
    "            offset_loss = raw_center_flat[0].sum() * 0.0\n",
    "            height_loss = raw_center_z_flat[0].sum() * 0.0\n",
    "        \n",
    "        total_offset_loss += offset_loss\n",
    "        total_height_loss += height_loss\n",
    "\n",
    "    avg_offset_loss = total_offset_loss / batch_count\n",
    "    avg_height_loss = total_height_loss / batch_count\n",
    "\n",
    "    return avg_offset_loss, avg_height_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lwh_loss(pred_dicts, gt_dicts):\n",
    "    \"\"\"\n",
    "    计算每个 batch (pred_dicts[i] 和 gt_dicts[i] 对应一个 batch) 的尺寸（长、宽、高）回归 L1 损失，\n",
    "    输入参数与 xyz_loss 完全一致。\n",
    "\n",
    "    pred_dicts: list，每个元素为一个 batch 的预测 dict，其中包含键:\n",
    "                - \"raw_dim\": 预测的尺寸张量，形状为 [3, H, W]\n",
    "    gt_dicts:   list，每个元素为一个 batch 的 GT dict，其中包含键:\n",
    "                - \"size\": GT 尺寸，形状为 [N, 3]  (包含长、宽和高)\n",
    "                - \"ind\":  每个有效目标在 BEV 下采样后的 flatten 索引，形状为 [N]\n",
    "                - \"reg_mask\": 有效目标的 mask，形状为 [N]\n",
    "\n",
    "    返回:\n",
    "       avg_size_loss: 平均尺寸回归损失（标量 tensor）\n",
    "    \"\"\"\n",
    "    total_size_loss = 0.0\n",
    "    batch_count = len(pred_dicts)\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        pred = pred_dicts[i]\n",
    "        gt = gt_dicts[i]\n",
    "        \n",
    "        # raw_dim: [3, H, W]\n",
    "        raw_dim = pred[\"raw_dim\"]\n",
    "        _, H, W = raw_dim.shape\n",
    "        # 扁平化为 [H*W, 3]\n",
    "        raw_dim_flat = raw_dim.view(3, -1).transpose(0, 1)\n",
    "        \n",
    "        # 确保 indices 为 long 类型\n",
    "        indices = gt[\"ind\"].to(torch.long)\n",
    "        \n",
    "        if indices.nelement() > 0:\n",
    "            # 从预测中采样出有效目标对应的尺寸预测： [N, 3]\n",
    "            pred_size = raw_dim_flat[indices]\n",
    "            gt_size = gt[\"size\"]  # [N, 3]\n",
    "            reg_mask = gt[\"reg_mask\"].float()\n",
    "            num_valid = reg_mask.sum() + 1e-4\n",
    "            \n",
    "            # 计算 L1 损失并归一化\n",
    "            loss_size = F.l1_loss(pred_size, gt_size, reduction=\"sum\") / num_valid\n",
    "        else:\n",
    "            # 若无有效目标，则返回与预测相关联的零 loss\n",
    "            loss_size = raw_dim_flat[0].sum() * 0.0\n",
    "        \n",
    "        total_size_loss += loss_size\n",
    "        \n",
    "    avg_size_loss = total_size_loss / batch_count\n",
    "    return avg_size_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_loss(pred_dicts, gt_dicts):\n",
    "    \"\"\"\n",
    "    计算每个 batch 的旋转回归 L1 损失，输入参数与 xyz_loss 和 lwh_loss 保持一致。\n",
    "    \n",
    "    输入：\n",
    "      pred_dicts: list，每个元素是一个 batch 的预测 dict，其中包含键：\n",
    "                  - \"raw_rot\": 预测的旋转张量，形状为 [2, H, W]，代表 sin 和 cos 的预测\n",
    "      gt_dicts:   list，每个元素是一个 batch 的 GT dict，其中包含键：\n",
    "                  - \"rot\": GT 的旋转角，形状为 [N]（标量角度）\n",
    "                  - \"ind\": GT 对应在 BEV 下采样后的 flatten 索引，形状为 [N]\n",
    "                  - \"reg_mask\": 有效目标 mask，形状为 [N]\n",
    "    \n",
    "    返回：\n",
    "      avg_rot_loss: 平均旋转回归损失（标量 tensor）\n",
    "    \"\"\"\n",
    "    total_rot_loss = 0.0\n",
    "    batch_count = len(pred_dicts)\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        pred = pred_dicts[i]\n",
    "        gt = gt_dicts[i]\n",
    "        \n",
    "        # raw_rot: [2, H, W] → flatten 成 [H*W, 2]\n",
    "        raw_rot = pred[\"raw_rot\"]\n",
    "        _, H, W = raw_rot.shape\n",
    "        raw_rot_flat = raw_rot.view(2, -1).transpose(0, 1)  # 形状: [H*W, 2]\n",
    "        \n",
    "        # 确保索引为 long 类型\n",
    "        indices = gt[\"ind\"].to(torch.long)\n",
    "        \n",
    "        if indices.nelement() > 0:\n",
    "            # 采样出预测的旋转值 [N, 2]\n",
    "            pred_rot = raw_rot_flat[indices]\n",
    "            \n",
    "            # GT 的旋转角以标量形式给出，转换为 sin 和 cos 组成的目标 [N, 2]\n",
    "            gt_rot = gt[\"rot\"].unsqueeze(-1)      # [N, 1]\n",
    "            gt_rot_targets = torch.cat([torch.sin(gt_rot), torch.cos(gt_rot)], dim=1)  # [N, 2]\n",
    "            \n",
    "            reg_mask = gt[\"reg_mask\"].float()\n",
    "            num_valid = reg_mask.sum() + 1e-4\n",
    "            \n",
    "            loss_rot = F.l1_loss(pred_rot, gt_rot_targets, reduction=\"sum\") / num_valid\n",
    "        else:\n",
    "            # 若无有效目标，则构造一个和预测相关联的零 loss，以确保 backward 时有 grad_fn\n",
    "            loss_rot = raw_rot_flat[0].sum() * 0.0\n",
    "        \n",
    "        total_rot_loss += loss_rot\n",
    "\n",
    "    avg_rot_loss = total_rot_loss / batch_count\n",
    "    return avg_rot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred_dicts, gt_dicts):\n",
    "    \"\"\"\n",
    "    计算每个 batch（pred_dicts[i] 和 gt_dicts[i] 对应一批）的 focal loss，\n",
    "    然后求平均总损失。\n",
    "\n",
    "    输入：\n",
    "      pred_dicts: list，每个元素为一个 batch 的预测 dict，\n",
    "                   其中包含键 'raw_hm'，形状为 [num_classes, H, W]\n",
    "      gt_dicts:   list，每个元素为一个 batch 的 GT dict，\n",
    "                   其中包含键 'heatmap'，形状为 [num_classes, H, W]\n",
    "\n",
    "    返回：\n",
    "      平均 focal loss（标量 tensor）\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    fc_ = 0.0\n",
    "    xyz_ = 0.0\n",
    "    lwh_ = 0.0\n",
    "    rot_ = 0.0\n",
    "    \n",
    "    batch_count = len(pred_dicts)\n",
    "    for i in range(batch_count):\n",
    "        # print(f\"计算第{i+1}个batch的 focal loss ...\")\n",
    "        # 获取该 batch 的预测热力图，注意需要先经过 sigmoid\n",
    "        \n",
    "        ######## 1 计算 focal loss ###########\n",
    "        pred_hm = torch.sigmoid(pred_dicts[i][\"raw_hm\"])  # [num_classes, H, W]\n",
    "        gt_hm = gt_dicts[i][\"heatmap\"]                    # [num_classes, H, W]\n",
    "\n",
    "        fc_loss = focal_loss(pred_hm, gt_hm)\n",
    "        total_loss += fc_loss\n",
    "        fc_ += fc_loss\n",
    "        ###################################\n",
    "        \n",
    "        \n",
    "        ######## 2 计算 xyz loss #############\n",
    "        xy_loss,z_loss = xyz_loss(pred_dicts,gt_dicts)\n",
    "        total_loss += xy_loss + z_loss\n",
    "        xyz_ += xy_loss + z_loss\n",
    "        ###################################\n",
    "        \n",
    "        ######## 3 计算 lwh loss #############\n",
    "        lwh_loss_ = lwh_loss(pred_dicts,gt_dicts)\n",
    "        total_loss += lwh_loss_\n",
    "        lwh_ += lwh_loss_\n",
    "        ###################################\n",
    "        \n",
    "        ######## 4 计算 rot loss #############\n",
    "        rot_loss_ = rot_loss(pred_dicts,gt_dicts)\n",
    "        total_loss += rot_loss_\n",
    "        rot_ += rot_loss_\n",
    "        ###################################\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_fc = fc_ / batch_count\n",
    "    avg_xyz = xyz_ / batch_count\n",
    "    avg_lwh = lwh_ / batch_count\n",
    "    avg_rot = rot_ / batch_count\n",
    "    \n",
    "    return dict(\n",
    "      total_loss = avg_loss,\n",
    "      fc_loss = avg_fc,\n",
    "      xyz_loss = avg_xyz,\n",
    "      lwh_loss = avg_lwh,\n",
    "      rot_loss = avg_rot\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 定义Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_metrics(pred_dicts, gt_dicts, center_thresh=1.0):\n",
    "    \"\"\"\n",
    "    计算每个 batch 的检测评估指标，包含：\n",
    "      - total_pred: 预测候选框总数\n",
    "      - total_gt:   GT 框总数\n",
    "      - total_matches: 成功匹配的 GT 框数\n",
    "      - avg_center_error: 匹配成功时中心位置的平均欧氏距离误差（仅基于 (x,y) 坐标）\n",
    "      - cls_accuracy: 匹配成功中预测类别正确的比率\n",
    "      - true_match: 匹配成功的预测框占总预测框的比例\n",
    "      - gt_box_recall: 匹配成功的 GT 框占总 GT 框的比例\n",
    "\n",
    "    匹配策略（简单策略）：\n",
    "      对于每个 GT 框（使用其 (x,y) 中心），计算与所有预测框中心之间的欧氏距离，\n",
    "      选择距离最小的预测框；若最小距离小于 center_thresh 且该预测框未被其它 GT 框匹配，则认为匹配成功。\n",
    "\n",
    "    输入：\n",
    "      pred_dicts: list，每个元素为一个 batch 的预测 dict，其中必须包含：\n",
    "                  - \"pred_boxes\": [N_pred, 7]，预测的 3D 框（格式：x, y, z, l, w, h, rot）\n",
    "                  - \"pred_labels\": [N_pred]，预测的类别标签\n",
    "      gt_dicts:   list，每个元素为一个 batch 的 GT dict，其中必须包含：\n",
    "                  - \"gt_boxes\": [N_gt, 7]，GT 框，格式同上\n",
    "                  - \"gt_labels\": [N_gt]，GT 类别标签\n",
    "    参数：\n",
    "      center_thresh: 匹配阈值，单位与 box 中 x,y 坐标一致（例如米）\n",
    "\n",
    "    返回：\n",
    "      metrics: dict，包含上述各项指标，其中还额外添加了：\n",
    "               - \"true_match\": total_matches / total_pred (预测匹配率)\n",
    "               - \"gt_box_recall\": total_matches / total_gt (GT 框的召回率)\n",
    "    \"\"\"\n",
    "    total_matches = 0\n",
    "    correct_cls = 0\n",
    "    total_center_error = 0.0\n",
    "    total_pred = 0\n",
    "    total_gt = 0\n",
    "    batch_count = len(pred_dicts)\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        pred = pred_dicts[i]\n",
    "        gt = gt_dicts[i]\n",
    "        \n",
    "        # 获取预测和 GT 的 box 与标签\n",
    "        pred_boxes = pred.get(\"pred_boxes\", torch.empty((0, 7)))  # [N_pred, 7]\n",
    "        pred_labels = pred.get(\"pred_labels\", torch.empty((0,), dtype=torch.long))  # [N_pred]\n",
    "        gt_boxes = gt.get(\"gt_boxes\", torch.empty((0, 7)))\n",
    "        gt_labels = gt.get(\"gt_labels\", torch.empty((0,), dtype=torch.long))\n",
    "        \n",
    "        total_pred += pred_boxes.shape[0]\n",
    "        total_gt += gt_boxes.shape[0]\n",
    "        \n",
    "        # 为简单起见，这里只计算 (x, y) 坐标之间的距离\n",
    "        matched_pred = set()  # 用于记录已经匹配的预测索引，保证一对一匹配\n",
    "        for j in range(gt_boxes.shape[0]):\n",
    "            gt_box = gt_boxes[j]\n",
    "            gt_center = gt_box[:2]  # 取 GT 的 x,y 中心\n",
    "            if pred_boxes.shape[0] == 0:\n",
    "                continue\n",
    "            # 预测框的中心也是取前两个数\n",
    "            pred_centers = pred_boxes[:, :2]  # [N_pred, 2]\n",
    "            # 计算每个预测与 GT 的欧氏距离\n",
    "            dists = torch.norm(pred_centers - gt_center.unsqueeze(0), dim=1)\n",
    "            min_val, min_idx = dists.min(0)\n",
    "            if min_val.item() < center_thresh and min_idx.item() not in matched_pred:\n",
    "                total_matches += 1\n",
    "                total_center_error += min_val.item()\n",
    "                if pred_labels[min_idx] == gt_labels[j]:\n",
    "                    correct_cls += 1\n",
    "                matched_pred.add(min_idx.item())\n",
    "    \n",
    "    if total_matches > 0:\n",
    "        avg_center_error = total_center_error / total_matches\n",
    "        cls_accuracy = correct_cls / total_matches\n",
    "    else:\n",
    "        avg_center_error = 0.0\n",
    "        cls_accuracy = 0.0\n",
    "\n",
    "    # 计算额外的指标, 注意避免除以 0\n",
    "    true_match = total_matches / total_pred if total_pred > 0 else 0.0\n",
    "    gt_box_recall = total_matches / total_gt if total_gt > 0 else 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_pred\": total_pred,\n",
    "        \"total_gt\": total_gt,\n",
    "        \"total_matches\": total_matches,\n",
    "        \"avg_center_error\": avg_center_error,\n",
    "        \"cls_accuracy\": cls_accuracy,\n",
    "        \"true_match\": true_match,\n",
    "        \"gt_box_recall\": gt_box_recall\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_size = [0.2, 0.2, 0.2]\n",
    "feature_map_stride = 1\n",
    "num_classes = 3\n",
    "batch_size = 1\n",
    "pc_range = [0, -44.8, -2, 224, 44.8, 4]\n",
    "lr = 1e-4\n",
    "\n",
    "\n",
    "dataset = PointCloudDataset(debug=False,\n",
    "                            voxel_size=voxel_size,\n",
    "                            feature_map_stride=feature_map_stride,\n",
    "                            pc_range=pc_range,\n",
    "                            num_classes=num_classes)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=False,collate_fn=myfunc,drop_last=True)\n",
    "\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False,collate_fn=myfunc,drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 这行注释掉\n",
    "\n",
    "model = CenterPoint(\n",
    "    voxel_size=voxel_size,\n",
    "    pc_range=pc_range,\n",
    "    feature_map_stride=feature_map_stride,\n",
    "    ).to(device)\n",
    "\n",
    "torch.cuda.empty_cache()  \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]['gt_boxes'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/80], Total Loss: 15927.5635, FC Loss: 15892.2208, XYZ Loss: 6.6563, LWH Loss: 27.1693, Rot Loss: 1.5174\n",
      "Epoch [1/100], Step [20/80], Total Loss: 14543.9091, FC Loss: 14510.5406, XYZ Loss: 4.8457, LWH Loss: 26.8658, Rot Loss: 1.6571\n",
      "Epoch [1/100], Step [30/80], Total Loss: 13131.1124, FC Loss: 13098.1703, XYZ Loss: 4.7399, LWH Loss: 26.5661, Rot Loss: 1.6363\n",
      "Epoch [1/100], Step [40/80], Total Loss: 12979.7051, FC Loss: 12947.1979, XYZ Loss: 4.2115, LWH Loss: 26.7085, Rot Loss: 1.5873\n",
      "Epoch [1/100], Step [50/80], Total Loss: 11574.8636, FC Loss: 11543.7156, XYZ Loss: 3.7929, LWH Loss: 25.7984, Rot Loss: 1.5567\n",
      "Epoch [1/100], Step [60/80], Total Loss: 10188.3676, FC Loss: 10157.3277, XYZ Loss: 4.0545, LWH Loss: 25.3890, Rot Loss: 1.5964\n",
      "Epoch [1/100], Step [70/80], Total Loss: 9152.6497, FC Loss: 9122.0277, XYZ Loss: 3.9694, LWH Loss: 25.0139, Rot Loss: 1.6387\n",
      "Epoch [1/100], Step [80/80], Total Loss: 9293.2780, FC Loss: 9262.9627, XYZ Loss: 3.6679, LWH Loss: 25.0349, Rot Loss: 1.6126\n",
      "Epoch [1/100], Average Total Loss: 9293.2780, Average FC Loss: 9262.9627, Average XYZ Loss: 3.6679, Average LWH Loss: 25.0349, Average Rot Loss: 1.6126\n",
      "Epoch [2/100], Step [10/80], Total Loss: 1243.7437, FC Loss: 1212.9106, XYZ Loss: 5.5899, LWH Loss: 23.8253, Rot Loss: 1.4179\n",
      "Epoch [2/100], Step [20/80], Total Loss: 1168.9394, FC Loss: 1140.3489, XYZ Loss: 3.7570, LWH Loss: 23.3112, Rot Loss: 1.5223\n",
      "Epoch [2/100], Step [30/80], Total Loss: 1143.6857, FC Loss: 1115.5410, XYZ Loss: 3.7105, LWH Loss: 22.9083, Rot Loss: 1.5259\n",
      "Epoch [2/100], Step [40/80], Total Loss: 1173.3638, FC Loss: 1145.7098, XYZ Loss: 3.2802, LWH Loss: 22.8513, Rot Loss: 1.5225\n",
      "Epoch [2/100], Step [50/80], Total Loss: 1092.8089, FC Loss: 1066.3393, XYZ Loss: 2.9149, LWH Loss: 22.0640, Rot Loss: 1.4906\n",
      "Epoch [2/100], Step [60/80], Total Loss: 995.7823, FC Loss: 969.2166, XYZ Loss: 3.2407, LWH Loss: 21.8428, Rot Loss: 1.4821\n",
      "Epoch [2/100], Step [70/80], Total Loss: 932.8205, FC Loss: 906.5232, XYZ Loss: 3.2196, LWH Loss: 21.5524, Rot Loss: 1.5253\n",
      "Epoch [2/100], Step [80/80], Total Loss: 1100.8158, FC Loss: 1074.6238, XYZ Loss: 2.9585, LWH Loss: 21.7451, Rot Loss: 1.4884\n",
      "Epoch [2/100], Average Total Loss: 1100.8158, Average FC Loss: 1074.6238, Average XYZ Loss: 2.9585, Average LWH Loss: 21.7451, Average Rot Loss: 1.4884\n",
      "Epoch [3/100], Step [10/80], Total Loss: 458.7523, FC Loss: 430.5039, XYZ Loss: 5.3620, LWH Loss: 21.4835, Rot Loss: 1.4029\n",
      "Epoch [3/100], Step [20/80], Total Loss: 452.1929, FC Loss: 426.6004, XYZ Loss: 3.5060, LWH Loss: 20.5873, Rot Loss: 1.4993\n",
      "Epoch [3/100], Step [30/80], Total Loss: 450.8415, FC Loss: 425.5500, XYZ Loss: 3.4834, LWH Loss: 20.3138, Rot Loss: 1.4944\n",
      "Epoch [3/100], Step [40/80], Total Loss: 458.8420, FC Loss: 433.8870, XYZ Loss: 3.0832, LWH Loss: 20.3816, Rot Loss: 1.4901\n",
      "Epoch [3/100], Step [50/80], Total Loss: 432.6009, FC Loss: 408.6092, XYZ Loss: 2.7470, LWH Loss: 19.7885, Rot Loss: 1.4563\n",
      "Epoch [3/100], Step [60/80], Total Loss: 401.0989, FC Loss: 376.9616, XYZ Loss: 3.0675, LWH Loss: 19.6183, Rot Loss: 1.4516\n",
      "Epoch [3/100], Step [70/80], Total Loss: 381.9407, FC Loss: 358.0535, XYZ Loss: 2.9546, LWH Loss: 19.5034, Rot Loss: 1.4291\n",
      "Epoch [3/100], Step [80/80], Total Loss: 465.4980, FC Loss: 441.7188, XYZ Loss: 2.7082, LWH Loss: 19.6907, Rot Loss: 1.3804\n",
      "Epoch [3/100], Average Total Loss: 465.4980, Average FC Loss: 441.7188, Average XYZ Loss: 2.7082, Average LWH Loss: 19.6907, Average Rot Loss: 1.3804\n",
      "Epoch [4/100], Step [10/80], Total Loss: 233.2723, FC Loss: 206.5656, XYZ Loss: 5.2125, LWH Loss: 20.1657, Rot Loss: 1.3285\n",
      "Epoch [4/100], Step [20/80], Total Loss: 236.4844, FC Loss: 212.3516, XYZ Loss: 3.3707, LWH Loss: 19.3452, Rot Loss: 1.4170\n",
      "Epoch [4/100], Step [30/80], Total Loss: 248.3844, FC Loss: 224.5972, XYZ Loss: 3.3409, LWH Loss: 19.0429, Rot Loss: 1.4033\n",
      "Epoch [4/100], Step [40/80], Total Loss: 255.4419, FC Loss: 232.1001, XYZ Loss: 2.9347, LWH Loss: 19.0087, Rot Loss: 1.3984\n",
      "Epoch [4/100], Step [50/80], Total Loss: 241.4130, FC Loss: 218.9413, XYZ Loss: 2.6053, LWH Loss: 18.4879, Rot Loss: 1.3785\n",
      "Epoch [4/100], Step [60/80], Total Loss: 225.7743, FC Loss: 203.1461, XYZ Loss: 2.9279, LWH Loss: 18.3252, Rot Loss: 1.3750\n",
      "Epoch [4/100], Step [70/80], Total Loss: 216.9976, FC Loss: 194.6623, XYZ Loss: 2.8505, LWH Loss: 18.1116, Rot Loss: 1.3732\n",
      "Epoch [4/100], Step [80/80], Total Loss: 264.2331, FC Loss: 242.0116, XYZ Loss: 2.6066, LWH Loss: 18.2816, Rot Loss: 1.3333\n",
      "Epoch [4/100], Average Total Loss: 264.2331, Average FC Loss: 242.0116, Average XYZ Loss: 2.6066, Average LWH Loss: 18.2816, Average Rot Loss: 1.3333\n",
      "Epoch [5/100], Step [10/80], Total Loss: 146.2204, FC Loss: 121.1227, XYZ Loss: 5.2117, LWH Loss: 18.5400, Rot Loss: 1.3460\n",
      "Epoch [5/100], Step [20/80], Total Loss: 151.7384, FC Loss: 129.5141, XYZ Loss: 3.2969, LWH Loss: 17.5480, Rot Loss: 1.3794\n",
      "Epoch [5/100], Step [30/80], Total Loss: 158.8969, FC Loss: 136.8606, XYZ Loss: 3.2713, LWH Loss: 17.3878, Rot Loss: 1.3771\n",
      "Epoch [5/100], Step [40/80], Total Loss: 165.6668, FC Loss: 144.0982, XYZ Loss: 2.8751, LWH Loss: 17.3218, Rot Loss: 1.3718\n",
      "Epoch [5/100], Step [50/80], Total Loss: 158.2247, FC Loss: 137.4120, XYZ Loss: 2.5496, LWH Loss: 16.9077, Rot Loss: 1.3554\n",
      "Epoch [5/100], Step [60/80], Total Loss: 149.7381, FC Loss: 128.6990, XYZ Loss: 2.8702, LWH Loss: 16.8164, Rot Loss: 1.3526\n",
      "Epoch [5/100], Step [70/80], Total Loss: 145.1642, FC Loss: 124.3569, XYZ Loss: 2.7742, LWH Loss: 16.6987, Rot Loss: 1.3344\n",
      "Epoch [5/100], Step [80/80], Total Loss: 176.3520, FC Loss: 155.6320, XYZ Loss: 2.5363, LWH Loss: 16.8922, Rot Loss: 1.2914\n",
      "Epoch [5/100], Average Total Loss: 176.3520, Average FC Loss: 155.6320, Average XYZ Loss: 2.5363, Average LWH Loss: 16.8922, Average Rot Loss: 1.2914\n",
      "Epoch [6/100], Step [10/80], Total Loss: 110.6344, FC Loss: 86.7060, XYZ Loss: 5.1628, LWH Loss: 17.4640, Rot Loss: 1.3016\n",
      "Epoch [6/100], Step [20/80], Total Loss: 112.1841, FC Loss: 91.0493, XYZ Loss: 3.2260, LWH Loss: 16.5799, Rot Loss: 1.3290\n",
      "Epoch [6/100], Step [30/80], Total Loss: 117.6843, FC Loss: 96.7355, XYZ Loss: 3.2003, LWH Loss: 16.4249, Rot Loss: 1.3237\n",
      "Epoch [6/100], Step [40/80], Total Loss: 122.7287, FC Loss: 102.2835, XYZ Loss: 2.8110, LWH Loss: 16.3084, Rot Loss: 1.3257\n",
      "Epoch [6/100], Step [50/80], Total Loss: 117.6784, FC Loss: 97.9217, XYZ Loss: 2.4934, LWH Loss: 15.9515, Rot Loss: 1.3118\n",
      "Epoch [6/100], Step [60/80], Total Loss: 112.0735, FC Loss: 92.0610, XYZ Loss: 2.8163, LWH Loss: 15.8871, Rot Loss: 1.3092\n",
      "Epoch [6/100], Step [70/80], Total Loss: 109.1121, FC Loss: 89.3233, XYZ Loss: 2.7207, LWH Loss: 15.7743, Rot Loss: 1.2937\n",
      "Epoch [6/100], Step [80/80], Total Loss: 131.1527, FC Loss: 111.4619, XYZ Loss: 2.4861, LWH Loss: 15.9536, Rot Loss: 1.2511\n",
      "Epoch [6/100], Average Total Loss: 131.1527, Average FC Loss: 111.4619, Average XYZ Loss: 2.4861, Average LWH Loss: 15.9536, Average Rot Loss: 1.2511\n",
      "Epoch [7/100], Step [10/80], Total Loss: 88.4677, FC Loss: 65.4821, XYZ Loss: 5.1369, LWH Loss: 16.5618, Rot Loss: 1.2869\n",
      "Epoch [7/100], Step [20/80], Total Loss: 89.3715, FC Loss: 69.2211, XYZ Loss: 3.1842, LWH Loss: 15.6700, Rot Loss: 1.2962\n",
      "Epoch [7/100], Step [30/80], Total Loss: 93.7935, FC Loss: 73.7062, XYZ Loss: 3.1625, LWH Loss: 15.6314, Rot Loss: 1.2933\n",
      "Epoch [7/100], Step [40/80], Total Loss: 97.5165, FC Loss: 77.9738, XYZ Loss: 2.7754, LWH Loss: 15.4745, Rot Loss: 1.2927\n",
      "Epoch [7/100], Step [50/80], Total Loss: 93.7238, FC Loss: 74.7990, XYZ Loss: 2.4625, LWH Loss: 15.1791, Rot Loss: 1.2832\n",
      "Epoch [7/100], Step [60/80], Total Loss: 89.7114, FC Loss: 70.4906, XYZ Loss: 2.7843, LWH Loss: 15.1547, Rot Loss: 1.2818\n",
      "Epoch [7/100], Step [70/80], Total Loss: 87.6032, FC Loss: 68.5711, XYZ Loss: 2.6898, LWH Loss: 15.0738, Rot Loss: 1.2685\n",
      "Epoch [7/100], Step [80/80], Total Loss: 104.3571, FC Loss: 85.4324, XYZ Loss: 2.4570, LWH Loss: 15.2390, Rot Loss: 1.2287\n",
      "Epoch [7/100], Average Total Loss: 104.3571, Average FC Loss: 85.4324, Average XYZ Loss: 2.4570, Average LWH Loss: 15.2390, Average Rot Loss: 1.2287\n",
      "Epoch [8/100], Step [10/80], Total Loss: 74.3142, FC Loss: 51.9688, XYZ Loss: 5.1120, LWH Loss: 15.9610, Rot Loss: 1.2723\n",
      "Epoch [8/100], Step [20/80], Total Loss: 74.4052, FC Loss: 54.8695, XYZ Loss: 3.1524, LWH Loss: 15.1082, Rot Loss: 1.2750\n",
      "Epoch [8/100], Step [30/80], Total Loss: 77.7607, FC Loss: 58.2618, XYZ Loss: 3.1257, LWH Loss: 15.1036, Rot Loss: 1.2695\n",
      "Epoch [8/100], Step [40/80], Total Loss: 80.6669, FC Loss: 61.7288, XYZ Loss: 2.7422, LWH Loss: 14.9265, Rot Loss: 1.2693\n",
      "Epoch [8/100], Step [50/80], Total Loss: 77.7138, FC Loss: 59.3262, XYZ Loss: 2.4331, LWH Loss: 14.6924, Rot Loss: 1.2621\n",
      "Epoch [8/100], Step [60/80], Total Loss: 74.7373, FC Loss: 56.0249, XYZ Loss: 2.7547, LWH Loss: 14.6970, Rot Loss: 1.2606\n",
      "Epoch [8/100], Step [70/80], Total Loss: 73.1561, FC Loss: 54.6171, XYZ Loss: 2.6564, LWH Loss: 14.6365, Rot Loss: 1.2461\n",
      "Epoch [8/100], Step [80/80], Total Loss: 86.4286, FC Loss: 68.0080, XYZ Loss: 2.4251, LWH Loss: 14.7877, Rot Loss: 1.2078\n",
      "Epoch [8/100], Average Total Loss: 86.4286, Average FC Loss: 68.0080, Average XYZ Loss: 2.4251, Average LWH Loss: 14.7877, Average Rot Loss: 1.2078\n",
      "Epoch [9/100], Step [10/80], Total Loss: 64.1554, FC Loss: 42.2551, XYZ Loss: 5.0969, LWH Loss: 15.5424, Rot Loss: 1.2610\n",
      "Epoch [9/100], Step [20/80], Total Loss: 63.5493, FC Loss: 44.4603, XYZ Loss: 3.1310, LWH Loss: 14.7003, Rot Loss: 1.2578\n",
      "Epoch [9/100], Step [30/80], Total Loss: 66.2466, FC Loss: 47.1486, XYZ Loss: 3.1051, LWH Loss: 14.7432, Rot Loss: 1.2498\n",
      "Epoch [9/100], Step [40/80], Total Loss: 68.6224, FC Loss: 50.1077, XYZ Loss: 2.7244, LWH Loss: 14.5379, Rot Loss: 1.2524\n",
      "Epoch [9/100], Step [50/80], Total Loss: 66.3036, FC Loss: 48.2850, XYZ Loss: 2.4165, LWH Loss: 14.3554, Rot Loss: 1.2467\n",
      "Epoch [9/100], Step [60/80], Total Loss: 64.0566, FC Loss: 45.6982, XYZ Loss: 2.7369, LWH Loss: 14.3768, Rot Loss: 1.2447\n",
      "Epoch [9/100], Step [70/80], Total Loss: 62.8174, FC Loss: 44.6199, XYZ Loss: 2.6371, LWH Loss: 14.3301, Rot Loss: 1.2302\n",
      "Epoch [9/100], Step [80/80], Total Loss: 73.6293, FC Loss: 55.5694, XYZ Loss: 2.4066, LWH Loss: 14.4614, Rot Loss: 1.1919\n",
      "Epoch [9/100], Average Total Loss: 73.6293, Average FC Loss: 55.5694, Average XYZ Loss: 2.4066, Average LWH Loss: 14.4614, Average Rot Loss: 1.1919\n",
      "Epoch [10/100], Step [10/80], Total Loss: 56.2597, FC Loss: 34.7129, XYZ Loss: 5.0811, LWH Loss: 15.2107, Rot Loss: 1.2550\n",
      "Epoch [10/100], Step [20/80], Total Loss: 55.0963, FC Loss: 36.3626, XYZ Loss: 3.1072, LWH Loss: 14.3802, Rot Loss: 1.2464\n",
      "Epoch [10/100], Step [30/80], Total Loss: 57.3779, FC Loss: 38.6188, XYZ Loss: 3.0796, LWH Loss: 14.4429, Rot Loss: 1.2365\n",
      "Epoch [10/100], Step [40/80], Total Loss: 59.4720, FC Loss: 41.3184, XYZ Loss: 2.7008, LWH Loss: 14.2155, Rot Loss: 1.2374\n",
      "Epoch [10/100], Step [50/80], Total Loss: 57.6866, FC Loss: 39.9749, XYZ Loss: 2.3959, LWH Loss: 14.0825, Rot Loss: 1.2332\n",
      "Epoch [10/100], Step [60/80], Total Loss: 56.0092, FC Loss: 37.9387, XYZ Loss: 2.7169, LWH Loss: 14.1223, Rot Loss: 1.2314\n",
      "Epoch [10/100], Step [70/80], Total Loss: 54.9979, FC Loss: 37.0809, XYZ Loss: 2.6157, LWH Loss: 14.0847, Rot Loss: 1.2165\n",
      "Epoch [10/100], Step [80/80], Total Loss: 64.0085, FC Loss: 46.2465, XYZ Loss: 2.3858, LWH Loss: 14.1964, Rot Loss: 1.1797\n",
      "Epoch [10/100], Average Total Loss: 64.0085, Average FC Loss: 46.2465, Average XYZ Loss: 2.3858, Average LWH Loss: 14.1964, Average Rot Loss: 1.1797\n",
      "Epoch [11/100], Step [10/80], Total Loss: 50.0007, FC Loss: 28.7483, XYZ Loss: 5.0696, LWH Loss: 14.9410, Rot Loss: 1.2418\n",
      "Epoch [11/100], Step [20/80], Total Loss: 48.5672, FC Loss: 30.1316, XYZ Loss: 3.0840, LWH Loss: 14.1183, Rot Loss: 1.2334\n",
      "Epoch [11/100], Step [30/80], Total Loss: 50.5937, FC Loss: 32.1122, XYZ Loss: 3.0574, LWH Loss: 14.2035, Rot Loss: 1.2207\n",
      "Epoch [11/100], Step [40/80], Total Loss: 52.5090, FC Loss: 34.6409, XYZ Loss: 2.6809, LWH Loss: 13.9616, Rot Loss: 1.2255\n",
      "Epoch [11/100], Step [50/80], Total Loss: 51.1370, FC Loss: 33.6662, XYZ Loss: 2.3779, LWH Loss: 13.8707, Rot Loss: 1.2223\n",
      "Epoch [11/100], Step [60/80], Total Loss: 49.8888, FC Loss: 32.0441, XYZ Loss: 2.6994, LWH Loss: 13.9252, Rot Loss: 1.2200\n",
      "Epoch [11/100], Step [70/80], Total Loss: 49.0400, FC Loss: 31.3404, XYZ Loss: 2.5991, LWH Loss: 13.8951, Rot Loss: 1.2054\n",
      "Epoch [11/100], Step [80/80], Total Loss: 56.6629, FC Loss: 39.1413, XYZ Loss: 2.3700, LWH Loss: 13.9829, Rot Loss: 1.1686\n",
      "Epoch [11/100], Average Total Loss: 56.6629, Average FC Loss: 39.1413, Average XYZ Loss: 2.3700, Average LWH Loss: 13.9829, Average Rot Loss: 1.1686\n",
      "Epoch [12/100], Step [10/80], Total Loss: 45.4371, FC Loss: 24.4131, XYZ Loss: 5.0533, LWH Loss: 14.7325, Rot Loss: 1.2381\n",
      "Epoch [12/100], Step [20/80], Total Loss: 43.7955, FC Loss: 25.5964, XYZ Loss: 3.0638, LWH Loss: 13.9133, Rot Loss: 1.2220\n",
      "Epoch [12/100], Step [30/80], Total Loss: 45.6044, FC Loss: 27.3397, XYZ Loss: 3.0381, LWH Loss: 14.0154, Rot Loss: 1.2112\n",
      "Epoch [12/100], Step [40/80], Total Loss: 47.3011, FC Loss: 29.6580, XYZ Loss: 2.6631, LWH Loss: 13.7666, Rot Loss: 1.2135\n",
      "Epoch [12/100], Step [50/80], Total Loss: 46.2100, FC Loss: 28.9248, XYZ Loss: 2.3622, LWH Loss: 13.7115, Rot Loss: 1.2116\n",
      "Epoch [12/100], Step [60/80], Total Loss: 45.2663, FC Loss: 27.5952, XYZ Loss: 2.6841, LWH Loss: 13.7778, Rot Loss: 1.2092\n",
      "Epoch [12/100], Step [70/80], Total Loss: 44.5288, FC Loss: 26.9996, XYZ Loss: 2.5841, LWH Loss: 13.7507, Rot Loss: 1.1945\n",
      "Epoch [12/100], Step [80/80], Total Loss: 51.0482, FC Loss: 33.7223, XYZ Loss: 2.3549, LWH Loss: 13.8130, Rot Loss: 1.1580\n",
      "Epoch [12/100], Average Total Loss: 51.0482, Average FC Loss: 33.7223, Average XYZ Loss: 2.3549, Average LWH Loss: 13.8130, Average Rot Loss: 1.1580\n",
      "Epoch [13/100], Step [10/80], Total Loss: 42.0384, FC Loss: 21.1838, XYZ Loss: 5.0450, LWH Loss: 14.5765, Rot Loss: 1.2331\n",
      "Epoch [13/100], Step [20/80], Total Loss: 40.1944, FC Loss: 22.1810, XYZ Loss: 3.0495, LWH Loss: 13.7502, Rot Loss: 1.2137\n",
      "Epoch [13/100], Step [30/80], Total Loss: 41.7952, FC Loss: 23.7028, XYZ Loss: 3.0277, LWH Loss: 13.8630, Rot Loss: 1.2017\n",
      "Epoch [13/100], Step [40/80], Total Loss: 43.2575, FC Loss: 25.8009, XYZ Loss: 2.6525, LWH Loss: 13.6005, Rot Loss: 1.2037\n",
      "Epoch [13/100], Step [50/80], Total Loss: 42.3692, FC Loss: 25.2355, XYZ Loss: 2.3525, LWH Loss: 13.5788, Rot Loss: 1.2024\n",
      "Epoch [13/100], Step [60/80], Total Loss: 41.6503, FC Loss: 24.1227, XYZ Loss: 2.6747, LWH Loss: 13.6527, Rot Loss: 1.2002\n",
      "Epoch [13/100], Step [70/80], Total Loss: 41.0040, FC Loss: 23.6195, XYZ Loss: 2.5756, LWH Loss: 13.6226, Rot Loss: 1.1863\n",
      "Epoch [13/100], Step [80/80], Total Loss: 46.6249, FC Loss: 29.4776, XYZ Loss: 2.3453, LWH Loss: 13.6524, Rot Loss: 1.1495\n",
      "Epoch [13/100], Average Total Loss: 46.6249, Average FC Loss: 29.4776, Average XYZ Loss: 2.3453, Average LWH Loss: 13.6524, Average Rot Loss: 1.1495\n",
      "Epoch [14/100], Step [10/80], Total Loss: 39.3849, FC Loss: 18.6858, XYZ Loss: 5.0352, LWH Loss: 14.4366, Rot Loss: 1.2274\n",
      "Epoch [14/100], Step [20/80], Total Loss: 37.3808, FC Loss: 19.5392, XYZ Loss: 3.0361, LWH Loss: 13.6021, Rot Loss: 1.2033\n",
      "Epoch [14/100], Step [30/80], Total Loss: 38.8053, FC Loss: 20.8706, XYZ Loss: 3.0151, LWH Loss: 13.7254, Rot Loss: 1.1941\n",
      "Epoch [14/100], Step [40/80], Total Loss: 40.0309, FC Loss: 22.7400, XYZ Loss: 2.6407, LWH Loss: 13.4549, Rot Loss: 1.1952\n",
      "Epoch [14/100], Step [50/80], Total Loss: 39.2886, FC Loss: 22.2897, XYZ Loss: 2.3417, LWH Loss: 13.4623, Rot Loss: 1.1949\n",
      "Epoch [14/100], Step [60/80], Total Loss: 38.7430, FC Loss: 21.3445, XYZ Loss: 2.6640, LWH Loss: 13.5417, Rot Loss: 1.1928\n",
      "Epoch [14/100], Step [70/80], Total Loss: 38.1735, FC Loss: 20.9227, XYZ Loss: 2.5661, LWH Loss: 13.5063, Rot Loss: 1.1785\n",
      "Epoch [14/100], Step [80/80], Total Loss: 43.0588, FC Loss: 26.0713, XYZ Loss: 2.3365, LWH Loss: 13.5093, Rot Loss: 1.1417\n",
      "Epoch [14/100], Average Total Loss: 43.0588, Average FC Loss: 26.0713, Average XYZ Loss: 2.3365, Average LWH Loss: 13.5093, Average Rot Loss: 1.1417\n",
      "Epoch [15/100], Step [10/80], Total Loss: 37.2591, FC Loss: 16.7059, XYZ Loss: 5.0202, LWH Loss: 14.3108, Rot Loss: 1.2222\n",
      "Epoch [15/100], Step [20/80], Total Loss: 35.1509, FC Loss: 17.4329, XYZ Loss: 3.0256, LWH Loss: 13.4921, Rot Loss: 1.2003\n",
      "Epoch [15/100], Step [30/80], Total Loss: 36.4350, FC Loss: 18.6213, XYZ Loss: 3.0063, LWH Loss: 13.6194, Rot Loss: 1.1879\n",
      "Epoch [15/100], Step [40/80], Total Loss: 37.4300, FC Loss: 20.2788, XYZ Loss: 2.6304, LWH Loss: 13.3326, Rot Loss: 1.1882\n",
      "Epoch [15/100], Step [50/80], Total Loss: 36.7937, FC Loss: 19.9092, XYZ Loss: 2.3327, LWH Loss: 13.3630, Rot Loss: 1.1889\n",
      "Epoch [15/100], Step [60/80], Total Loss: 36.3844, FC Loss: 19.0959, XYZ Loss: 2.6555, LWH Loss: 13.4459, Rot Loss: 1.1872\n",
      "Epoch [15/100], Step [70/80], Total Loss: 35.9115, FC Loss: 18.7466, XYZ Loss: 2.5630, LWH Loss: 13.4267, Rot Loss: 1.1752\n",
      "Epoch [15/100], Step [80/80], Total Loss: 40.1714, FC Loss: 23.3099, XYZ Loss: 2.3330, LWH Loss: 13.3936, Rot Loss: 1.1348\n",
      "Epoch [15/100], Average Total Loss: 40.1714, Average FC Loss: 23.3099, Average XYZ Loss: 2.3330, Average LWH Loss: 13.3936, Average Rot Loss: 1.1348\n",
      "Epoch [16/100], Step [10/80], Total Loss: 35.5648, FC Loss: 15.0934, XYZ Loss: 5.0287, LWH Loss: 14.2253, Rot Loss: 1.2175\n",
      "Epoch [16/100], Step [20/80], Total Loss: 33.3313, FC Loss: 15.7110, XYZ Loss: 3.0310, LWH Loss: 13.3981, Rot Loss: 1.1913\n",
      "Epoch [16/100], Step [30/80], Total Loss: 34.5121, FC Loss: 16.7929, XYZ Loss: 3.0081, LWH Loss: 13.5301, Rot Loss: 1.1810\n",
      "Epoch [16/100], Step [40/80], Total Loss: 35.3061, FC Loss: 18.2579, XYZ Loss: 2.6279, LWH Loss: 13.2383, Rot Loss: 1.1820\n",
      "Epoch [16/100], Step [50/80], Total Loss: 34.7518, FC Loss: 17.9525, XYZ Loss: 2.3291, LWH Loss: 13.2867, Rot Loss: 1.1835\n",
      "Epoch [16/100], Step [60/80], Total Loss: 34.4479, FC Loss: 17.2433, XYZ Loss: 2.6499, LWH Loss: 13.3733, Rot Loss: 1.1815\n",
      "Epoch [16/100], Step [70/80], Total Loss: 34.0457, FC Loss: 16.9394, XYZ Loss: 2.5387, LWH Loss: 13.3974, Rot Loss: 1.1702\n",
      "Epoch [16/100], Step [80/80], Total Loss: 37.7710, FC Loss: 21.0130, XYZ Loss: 2.3124, LWH Loss: 13.3154, Rot Loss: 1.1302\n",
      "Epoch [16/100], Average Total Loss: 37.7710, Average FC Loss: 21.0130, Average XYZ Loss: 2.3124, Average LWH Loss: 13.3154, Average Rot Loss: 1.1302\n",
      "Epoch [17/100], Step [10/80], Total Loss: 34.0795, FC Loss: 13.7561, XYZ Loss: 4.9982, LWH Loss: 14.1157, Rot Loss: 1.2096\n",
      "Epoch [17/100], Step [20/80], Total Loss: 31.7711, FC Loss: 14.2854, XYZ Loss: 3.0017, LWH Loss: 13.2945, Rot Loss: 1.1895\n",
      "Epoch [17/100], Step [30/80], Total Loss: 32.8637, FC Loss: 15.2640, XYZ Loss: 2.9861, LWH Loss: 13.4363, Rot Loss: 1.1772\n",
      "Epoch [17/100], Step [40/80], Total Loss: 33.4809, FC Loss: 16.5627, XYZ Loss: 2.6080, LWH Loss: 13.1332, Rot Loss: 1.1770\n",
      "Epoch [17/100], Step [50/80], Total Loss: 33.0001, FC Loss: 16.3104, XYZ Loss: 2.3123, LWH Loss: 13.1985, Rot Loss: 1.1789\n",
      "Epoch [17/100], Step [60/80], Total Loss: 32.7904, FC Loss: 15.6896, XYZ Loss: 2.6352, LWH Loss: 13.2883, Rot Loss: 1.1774\n",
      "Epoch [17/100], Step [70/80], Total Loss: 32.4261, FC Loss: 15.4292, XYZ Loss: 2.5267, LWH Loss: 13.3042, Rot Loss: 1.1660\n",
      "Epoch [17/100], Step [80/80], Total Loss: 35.6947, FC Loss: 19.0891, XYZ Loss: 2.2972, LWH Loss: 13.1819, Rot Loss: 1.1265\n",
      "Epoch [17/100], Average Total Loss: 35.6947, Average FC Loss: 19.0891, Average XYZ Loss: 2.2972, Average LWH Loss: 13.1819, Average Rot Loss: 1.1265\n",
      "Epoch [18/100], Step [10/80], Total Loss: 32.8810, FC Loss: 12.6298, XYZ Loss: 5.0092, LWH Loss: 14.0258, Rot Loss: 1.2163\n",
      "Epoch [18/100], Step [20/80], Total Loss: 30.4967, FC Loss: 13.0880, XYZ Loss: 3.0077, LWH Loss: 13.2161, Rot Loss: 1.1849\n",
      "Epoch [18/100], Step [30/80], Total Loss: 31.4960, FC Loss: 13.9775, XYZ Loss: 2.9885, LWH Loss: 13.3553, Rot Loss: 1.1747\n",
      "Epoch [18/100], Step [40/80], Total Loss: 31.9626, FC Loss: 15.1369, XYZ Loss: 2.6073, LWH Loss: 13.0445, Rot Loss: 1.1739\n",
      "Epoch [18/100], Step [50/80], Total Loss: 31.5395, FC Loss: 14.9272, XYZ Loss: 2.3109, LWH Loss: 13.1252, Rot Loss: 1.1762\n",
      "Epoch [18/100], Step [60/80], Total Loss: 31.4025, FC Loss: 14.3800, XYZ Loss: 2.6337, LWH Loss: 13.2136, Rot Loss: 1.1752\n",
      "Epoch [18/100], Step [70/80], Total Loss: 31.0639, FC Loss: 14.1582, XYZ Loss: 2.5279, LWH Loss: 13.2151, Rot Loss: 1.1627\n",
      "Epoch [18/100], Step [80/80], Total Loss: 33.8978, FC Loss: 17.4711, XYZ Loss: 2.3007, LWH Loss: 13.0007, Rot Loss: 1.1253\n",
      "Epoch [18/100], Average Total Loss: 33.8978, Average FC Loss: 17.4711, Average XYZ Loss: 2.3007, Average LWH Loss: 13.0007, Average Rot Loss: 1.1253\n",
      "Epoch [19/100], Step [10/80], Total Loss: 31.8108, FC Loss: 11.6807, XYZ Loss: 5.0017, LWH Loss: 13.9282, Rot Loss: 1.2002\n",
      "Epoch [19/100], Step [20/80], Total Loss: 29.4176, FC Loss: 12.0900, XYZ Loss: 3.0072, LWH Loss: 13.1326, Rot Loss: 1.1878\n",
      "Epoch [19/100], Step [30/80], Total Loss: 30.3437, FC Loss: 12.9029, XYZ Loss: 2.9882, LWH Loss: 13.2784, Rot Loss: 1.1742\n",
      "Epoch [19/100], Step [40/80], Total Loss: 30.6814, FC Loss: 13.9453, XYZ Loss: 2.6059, LWH Loss: 12.9578, Rot Loss: 1.1724\n",
      "Epoch [19/100], Step [50/80], Total Loss: 30.3057, FC Loss: 13.7674, XYZ Loss: 2.3098, LWH Loss: 13.0536, Rot Loss: 1.1750\n",
      "Epoch [19/100], Step [60/80], Total Loss: 30.2306, FC Loss: 13.2815, XYZ Loss: 2.6323, LWH Loss: 13.1432, Rot Loss: 1.1736\n",
      "Epoch [19/100], Step [70/80], Total Loss: 29.8965, FC Loss: 13.0932, XYZ Loss: 2.5315, LWH Loss: 13.1117, Rot Loss: 1.1601\n",
      "Epoch [19/100], Step [80/80], Total Loss: 32.5559, FC Loss: 16.1296, XYZ Loss: 2.3071, LWH Loss: 12.9913, Rot Loss: 1.1279\n",
      "Epoch [19/100], Average Total Loss: 32.5559, Average FC Loss: 16.1296, Average XYZ Loss: 2.3071, Average LWH Loss: 12.9913, Average Rot Loss: 1.1279\n",
      "Epoch [20/100], Step [10/80], Total Loss: 31.0324, FC Loss: 10.8798, XYZ Loss: 4.9971, LWH Loss: 13.9497, Rot Loss: 1.2058\n",
      "Epoch [20/100], Step [20/80], Total Loss: 28.5608, FC Loss: 11.2420, XYZ Loss: 2.9962, LWH Loss: 13.1448, Rot Loss: 1.1777\n",
      "Epoch [20/100], Step [30/80], Total Loss: 29.4328, FC Loss: 12.0058, XYZ Loss: 2.9809, LWH Loss: 13.2746, Rot Loss: 1.1714\n",
      "Epoch [20/100], Step [40/80], Total Loss: 29.7004, FC Loss: 12.9521, XYZ Loss: 2.5976, LWH Loss: 12.9814, Rot Loss: 1.1693\n",
      "Epoch [20/100], Step [50/80], Total Loss: 29.3432, FC Loss: 12.8012, XYZ Loss: 2.3025, LWH Loss: 13.0675, Rot Loss: 1.1719\n",
      "Epoch [20/100], Step [60/80], Total Loss: 29.3260, FC Loss: 12.3676, XYZ Loss: 2.6261, LWH Loss: 13.1615, Rot Loss: 1.1708\n",
      "Epoch [20/100], Step [70/80], Total Loss: 29.0396, FC Loss: 12.2075, XYZ Loss: 2.5313, LWH Loss: 13.1423, Rot Loss: 1.1584\n",
      "Epoch [20/100], Step [80/80], Total Loss: 31.5779, FC Loss: 14.9325, XYZ Loss: 2.3077, LWH Loss: 13.2137, Rot Loss: 1.1240\n",
      "Epoch [20/100], Average Total Loss: 31.5779, Average FC Loss: 14.9325, Average XYZ Loss: 2.3077, Average LWH Loss: 13.2137, Average Rot Loss: 1.1240\n",
      "Epoch [21/100], Step [10/80], Total Loss: 30.2024, FC Loss: 10.1736, XYZ Loss: 4.9782, LWH Loss: 13.8597, Rot Loss: 1.1909\n",
      "Epoch [21/100], Step [20/80], Total Loss: 27.7155, FC Loss: 10.5031, XYZ Loss: 2.9777, LWH Loss: 13.0675, Rot Loss: 1.1673\n",
      "Epoch [21/100], Step [30/80], Total Loss: 28.5039, FC Loss: 11.2022, XYZ Loss: 2.9625, LWH Loss: 13.1804, Rot Loss: 1.1588\n",
      "Epoch [21/100], Step [40/80], Total Loss: 28.6597, FC Loss: 12.0543, XYZ Loss: 2.5828, LWH Loss: 12.8624, Rot Loss: 1.1603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 更新参数\u001b[39;00m\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m running_fc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py:84\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     81\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 训练模式\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    running_fc = 0.0\n",
    "    running_xyz = 0.0\n",
    "    running_lwh = 0.0\n",
    "    running_rot = 0.0\n",
    "    \n",
    "    for i, (pointclouds, gt_dicts, lengths) in enumerate(trainloader):\n",
    "        # 将点云数据和对应的标签移到 CPU 上（如果不调用 .cuda() 则本来就在 CPU）\n",
    "        pointclouds = move_to_gpu(pointclouds)\n",
    "        gt_dicts = move_to_gpu(gt_dicts)\n",
    "        \n",
    "        print_dict_tensors_size(gt_dicts)\n",
    "        \n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        \n",
    "        # 前向传播\n",
    "        pred_dicts = model(pointclouds)\n",
    "\n",
    "        # 计算损失\n",
    "        loss_dict = compute_loss(pred_dicts, gt_dicts)\n",
    "        loss = loss_dict['total_loss']\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_fc += loss_dict['fc_loss'].item()\n",
    "        running_xyz += loss_dict['xyz_loss'].item()\n",
    "        running_lwh += loss_dict['lwh_loss'].item()\n",
    "        running_rot += loss_dict['rot_loss'].item()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Total Loss: {running_loss / (i + 1):.4f}, FC Loss: {running_fc / (i+1):.4f}, XYZ Loss: {running_xyz / (i+1):.4f}, LWH Loss: {running_lwh / (i+1):.4f}, Rot Loss: {running_rot / (i+1):.4f}\")\n",
    "            \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Total Loss: {running_loss / len(trainloader):.4f}, Average FC Loss: {running_fc / len(trainloader):.4f}, Average XYZ Loss: {running_xyz / len(trainloader):.4f}, Average LWH Loss: {running_lwh / len(trainloader):.4f}, Average Rot Loss: {running_rot / len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''WANDDB\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import wandb  # pip install wandb\n",
    "from pprint import pprint\n",
    "\n",
    "# 初始化 wandb，指定项目名称和一些配置参数\n",
    "wandb.init(project=\"your_project_name\", config={\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    # 其它配置...\n",
    "})\n",
    "\n",
    "# 如果你之前用 move_to_gpu 函数，此处不需要修改\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 训练模式\n",
    "    running_loss = 0.0\n",
    "    running_fc = 0.0\n",
    "    running_xyz = 0.0\n",
    "    running_lwh = 0.0\n",
    "    running_rot = 0.0\n",
    "\n",
    "    for i, (pointclouds, gt_dicts, lengths) in enumerate(trainloader):\n",
    "        pointclouds = move_to_gpu(pointclouds)\n",
    "        gt_dicts = move_to_gpu(gt_dicts)\n",
    "        \n",
    "        # 可以打印 GT 信息尺寸以作调试\n",
    "        # print_dict_tensors_size(gt_dicts)\n",
    "        \n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        \n",
    "        # 前向传播\n",
    "        pred_dicts = model(pointclouds)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss_dict = compute_loss(pred_dicts, gt_dicts)\n",
    "        loss = loss_dict['total_loss']\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_fc += loss_dict['fc_loss'].item()\n",
    "        running_xyz += loss_dict['xyz_loss'].item()\n",
    "        running_lwh += loss_dict['lwh_loss'].item()\n",
    "        running_rot += loss_dict['rot_loss'].item()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            # 打印当前平均损失\n",
    "            avg_total = running_loss / (i+1)\n",
    "            avg_fc = running_fc / (i+1)\n",
    "            avg_xyz = running_xyz / (i+1)\n",
    "            avg_lwh = running_lwh / (i+1)\n",
    "            avg_rot = running_rot / (i+1)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], \"\n",
    "                  f\"Total Loss: {avg_total:.4f}, FC Loss: {avg_fc:.4f}, \"\n",
    "                  f\"XYZ Loss: {avg_xyz:.4f}, LWH Loss: {avg_lwh:.4f}, Rot Loss: {avg_rot:.4f}\")\n",
    "            \n",
    "            # 同步记录到 wandb\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch + 1,\n",
    "                \"Step\": i + 1,\n",
    "                \"Total Loss\": avg_total,\n",
    "                \"FC Loss\": avg_fc,\n",
    "                \"XYZ Loss\": avg_xyz,\n",
    "                \"LWH Loss\": avg_lwh,\n",
    "                \"Rot Loss\": avg_rot\n",
    "            }, step=epoch * len(trainloader) + i)\n",
    "\n",
    "    avg_total_epoch = running_loss / len(trainloader)\n",
    "    avg_fc_epoch = running_fc / len(trainloader)\n",
    "    avg_xyz_epoch = running_xyz / len(trainloader)\n",
    "    avg_lwh_epoch = running_lwh / len(trainloader)\n",
    "    avg_rot_epoch = running_rot / len(trainloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Total Loss: {avg_total_epoch:.4f}, \"\n",
    "          f\"FC Loss: {avg_fc_epoch:.4f}, XYZ Loss: {avg_xyz_epoch:.4f}, \"\n",
    "          f\"LWH Loss: {avg_lwh_epoch:.4f}, Rot Loss: {avg_rot_epoch:.4f}\")\n",
    "    \n",
    "    # 每个 epoch 后记录一次\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Average Total Loss\": avg_total_epoch,\n",
    "        \"Average FC Loss\": avg_fc_epoch,\n",
    "        \"Average XYZ Loss\": avg_xyz_epoch,\n",
    "        \"Average LWH Loss\": avg_lwh_epoch,\n",
    "        \"Average Rot Loss\": avg_rot_epoch\n",
    "    }, step=(epoch+1) * len(trainloader))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
